---
title: 'From Text to Intelligence: My Fresno DevFest Keynote on Embeddings'
date: '2025-07-06 00:00:00'
tags: ['ai', 'devfest', 'fresno']
draft: false
summary: 'Expanded recap of my Fresno DevFest keynote using personal notes and slides to explore embeddings, vector databases, and AI-powered search.'
images: []
---

# ğŸ¤ From Text to Intelligence: My Fresno DevFest Keynote on Embeddings

At **Fresno DevFest **, I had the opportunity to present a keynote titled _â€œLeveraging LLMs for Search: Exploring Embeddings.â€_ In this talk, I walked through how embeddingsâ€”one of the most powerful tools behind LLMsâ€”enable smarter, more meaningful interactions with data.

## ğŸ¤– What Are Embeddings?

Embeddings show us what the model "sees" in a piece of data. By converting text (or even images) into arrays of numbers, they help LLMs understand relationships, meaning, and context. 

These vector representations allow us to:

- Perform semantic search  
- Group similar content via clustering  
- Power intelligent classification  
- Provide focused context to LLMs without full fine-tuning  

> ğŸ§® _â€œKingâ€ â€“ â€œManâ€ + â€œWomanâ€ = â€œQueenâ€_ is a famous example of how relationships are preserved in vector space.

ğŸ”— Try word2vec live: [https://turbomaze.github.io/word2vecjson/](https://turbomaze.github.io/word2vecjson/)

## ğŸ›  Creating Embeddings: APIs, OSS, and LangChain

We explored multiple paths for generating embeddings:

- **OpenAI Embedding API** â€” Simple but vulnerable to deprecations or API changes  
- **LLAMA2 + Huggingface** â€” Open-source, hostable, customizable  
- **LangChain** â€” A powerful abstraction layer for connecting and switching between models

LangChain simplifies embedding workflows by offering consistent interfaces, while Huggingface offers plug-and-play flexibility with models like CodeLlama and word2vec.

## ğŸ§ª Live Demo: Pokedex Semantic Search

My personal dive into embeddings began with a simple semantic search appâ€”**a Pokedex**. Rather than keyword-based querying, we can now ask things like "electric rodent" and get **Pikachu** back thanks to vector-based matching.

We used:

- ğŸ˜ `pgvector` with Postgres
- ğŸ” ElasticSearch for full-text + vector hybrid search

SQL-style similarity query:
```sql
SELECT 1 - (embedding <=> '[3,1,2]') AS cosine_similarity FROM items;
```

ğŸ‘‰ Check out the live demo: [https://pokedex-seven-sigma.vercel.app](https://pokedex-seven-sigma.vercel.app)

## ğŸ”® What's Next?

Exciting improvements are on the horizon:

- **OpenAI's CLIP** â€” Embed both images and text in the same vector space  
- **In-browser embeddings** â€” No API required: [CLIP Demo](https://observablehq.com/@simonw/openai-clip-in-a-browser)  
- **RAG Pipelines** â€” Use embeddings to inject relevant context into LLM queries  
- **Smaller, local models** â€” Embeddings and chat agents on your laptop  

This opens up creative possibilities across search, personalization, and AI workflowsâ€”especially for those working with **private data**.

## ğŸ§  Fine-Tuning vs Context

Embedding-powered retrieval enables a **context-first** approach. Instead of fine-tuning large models, we can:

- Narrow the context  
- Inject relevant, up-to-date info  
- Keep cost and tokens under control  

---

The future of search and AI is contextual, efficient, and deeply personal. Embeddings help make that future possibleâ€”and accessible.

